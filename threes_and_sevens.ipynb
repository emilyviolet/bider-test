{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super-duper quick 'n' dirt character analysis, from scratch\n",
    "My attempt at following along with the []\"Hello, Deep Learning\" tutorial](https://berthub.eu/articles/posts/hello-deep-learning/) by Bert Hubert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data\n",
    "Before we can do any maths, we need to open the [EMNIST dataset](https://www.nist.gov/itl/products-and-services/emnist-dataset) in a form\n",
    "Python can use. NIST only supplies the data in a janky, custom binary format (or MatLab format) and parsing it is a mug's game. Instead, let's just use PyTorch's built-in methods to download and parse the dataset (documentation can be found [here](\"https://pytorch.org/vision/main/generated/torchvision.datasets.EMNIST.html#torchvision.datasets.EMNIST\")):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = torchvision.datasets.EMNIST(\"./emnist\",split='digits', train=True, download=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is stored in a TorchVision `image` type. Let's inspect its members:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(training_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the data by calling `dataset.data` and the labels by calling `dataset.targets`, both of which are PyTorch `Tensor` objects. We can also convert these `Tensor`s to NumPy arrays by calling their `.numpy()` method (working with plain numpy arrays for now since I don't want to use PyTorch *too* much during the learning stage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = training_dataset.data.numpy()\n",
    "image_labels = training_dataset.targets.numpy()\n",
    "num_images = image_labels.shape[0]\n",
    "print(f\"Read {num_images} training images.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go through and select just the images that correspond to the \"threes\" and \"sevens\". The image data consists of a 3D array, the first index corresponding to the \"label index\", while the second two elements are the x- and y-pixel values (which are always 28x28 in size). The labels are a flat array, so `image_data[i]` corresponds to `image_labels[i]`. Let's put this into action by picking out just the images we want and the plotting a sample one using matplotlib (don't forget that the training data are rotated and mirrored relative to how a human would write it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threes_indices = []\n",
    "sevens_indices = []\n",
    "for i in range(len(image_labels)):\n",
    "  if(image_labels[i] == 3):\n",
    "    threes_indices.append(i)\n",
    "  elif(image_labels[i] == 7):\n",
    "    sevens_indices.append(i)\n",
    "\n",
    "threes = image_data[threes_indices]\n",
    "sevens = image_data[sevens_indices]\n",
    "num_threes = threes.shape[0]\n",
    "num_sevens = sevens.shape[0]\n",
    "\n",
    "# Quick 'n' dirty plot to check it worked (just one because I can't be bothered to set up multiplots right now)\n",
    "plt.imshow(sevens[1], interpolation=\"nearest\", cmap=\"gray\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neato! Now we can actually start processing the data. \n",
    "\n",
    "## Processing the training data\n",
    "We'll want to calculate two arrays containing the average brightness at each pixel for both the \"seven\" and \"three\" datasets, then compute the *difference* between these two averages to compute a \"weights\" matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_three = np.mean(threes, axis=0)\n",
    "avg_seven = np.mean(sevens, axis=0)\n",
    "\n",
    "# Compute average difference, then normalise by the number of samples\n",
    "avg_diff = (avg_seven - avg_three)/num_images\n",
    "plt.imshow(avg_diff, interpolation=\"nearest\", cmap=\"gray\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying images\n",
    "Now, the general idea is that we can use our weights matrix of average differences as a classifier: pixels with a large positive values is more likely to be associated with a \"seven\", while negative values are more likely to be associated with a \"three\". We can \"score\" each image by computing its element-wise product with the weights matrix and calculating the sum of all pixels in the resulting array - this gives us a measure of \"threeness\" or \"sevenness\". For example, using the seven from above (`sevens[1]`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.zeros((28,28), np.float32)\n",
    "np.multiply(sevens[1], avg_diff, out=tmp)\n",
    "mean = np.sum(tmp)\n",
    "print(f\"Sevenness = {mean}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat this for all our training data and calculate the average \"threeness\" of all \"threes\" and the \"sevenness\" of all \"sevens\". \n",
    "This measure will not in general be symmetric around zero, so we can compute the average of the above \"threeness\" and \"sevenness\" to get what's called a *bias*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_threeness = 0.0;\n",
    "avg_sevenness = 0.0\n",
    "# Temporary array to hold the weighted pixels. Totally fine to overrite this after we calculate each average\n",
    "tmp = np.zeros((28,28), np.float32)\n",
    "\n",
    "# Threes\n",
    "for i in range(num_threes):\n",
    "  np.multiply(threes[i], avg_diff, out=tmp)\n",
    "  avg_threeness += np.sum(tmp)\n",
    "\n",
    "avg_threeness /= num_threes\n",
    "\n",
    "# Sevens\n",
    "for i in range(num_sevens):\n",
    "  np.multiply(sevens[i], avg_diff, out=tmp)\n",
    "  avg_sevenness += np.sum(tmp)\n",
    "\n",
    "avg_sevenness /= num_sevens\n",
    "\n",
    "bias = (avg_threeness - avg_sevenness)/2\n",
    "print(f\"Average threeness = {avg_threeness}\\nAverage sevenness={avg_sevenness}\\nBias = {bias}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating the model\n",
    "With the bias calculated, we can do this procedure on the validation data and if a given image's pixel-sum (the residue) is larger than the bias, it's a seve, otherwise it's a three.\n",
    "\n",
    "Let's give this a try. First, we have to load the testing data and extract the threes and sevens like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download testing data\n",
    "testing_dataset = torchvision.datasets.EMNIST(\"./emnist\",split='digits', train=False, download=True)\n",
    "testing_data = training_dataset.data.numpy()\n",
    "testing_labels = training_dataset.targets.numpy()\n",
    "num_training = testing_labels.shape[0]\n",
    "\n",
    "# Extract threes and sevens\n",
    "threes_indices = []\n",
    "sevens_indices = []\n",
    "for i in range(len(image_labels)):\n",
    "  if(testing_labels[i] == 3):\n",
    "    threes_indices.append(i)\n",
    "  elif(testing_labels[i] == 7):\n",
    "    sevens_indices.append(i)\n",
    "threes = testing_data[threes_indices]\n",
    "num_threes = threes.shape[0]\n",
    "num_sevens = sevens.shape[0]\n",
    "sevens = image_data[sevens_indices]\n",
    "\n",
    "# Now run through the threes, classify the image and check whether it's correct or not\n",
    "tmp = np.zeros((28,28), np.float32)\n",
    "bad_predictions = {3: [], 7:[]}\n",
    "threes_accuracy = 0.0\n",
    "\n",
    "for i in range(num_threes):\n",
    "  np.multiply(threes[i], avg_diff, out=tmp)\n",
    "  if np.sum(tmp) <= bias:\n",
    "    threes_accuracy += 1\n",
    "  else:\n",
    "    bad_predictions[3].append(i)\n",
    "\n",
    "threes_accuracy /= num_threes\n",
    "\n",
    "# And again for sevens\n",
    "sevens_accuracy = 0.0\n",
    "\n",
    "for i in range(num_sevens):\n",
    "  np.multiply(sevens[i], avg_diff, out=tmp)\n",
    "  if np.sum(tmp) > bias:\n",
    "    sevens_accuracy += 1\n",
    "    # Check if it's actually correct!\n",
    "  else:\n",
    "    # Save the wrong predictions to plot later\n",
    "    bad_predictions[7].append(i)\n",
    "sevens_accuracy /= num_sevens\n",
    "\n",
    "print(f\"Classifier accuracy for validation dataset:\\nThrees: {threes_accuracy}%\\nSevens: {sevens_accuracy}%\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's draw some of the figures that our classifier got wrong, which should help give an idea of what it's \"thinking\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some bad predictions on separate subplots\n",
    "fig, axes = plt.subplots(2, 10)\n",
    "for i in range(10):\n",
    "  #plt.subplot()\n",
    "  #plt.imshow(threes[i])\n",
    "  axes[0][i].imshow(threes[bad_predictions[3][i]])\n",
    "for i in range(10):\n",
    "  axes[1][i].imshow(sevens[bad_predictions[7][i]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty good accuracy for such a dumb model! It only works for threes vs sevens and won't work with other characters, but that's the price we pay for sticking to elementary linear algebra. If we *do* want to generalise, then we'll need to get more complex, but this basic process is (conceptually) at the heart of how a neural network classifier works."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
